{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9_seq2seq.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"aoBrxo-AGoFx","colab_type":"text"},"cell_type":"markdown","source":["# Attention"]},{"metadata":{"id":"b2vZERiLR_Rb","colab_type":"text"},"cell_type":"markdown","source":["seq2seq — "]},{"metadata":{"id":"e2yuNNCOGoFx","colab_type":"text"},"cell_type":"markdown","source":["С 2017 года в самом передовом нейросетевом NLP почти всегда используется в каком-то виде механизм attention (англ. «внимание»).\n","\n","В Google даже сделали сеть для перевода («Transformer»), которая не использует каких-либо скрытых состояний, которая сейчас считается лучшей в мире (см. Attention is All You Need).\n","\n","Его можно рассмотреть как базу данных: у нас есть какие-то данные (векторизованные, одного размера), у них есть ключи, и нам по запросу нужно эти данные вернуть.\n","\n","-- TODO --"]},{"metadata":{"id":"B3xLiRWoR_Rg","colab_type":"text"},"cell_type":"markdown","source":["# Transformer\n","\n","[не очень понятная схема из статьи]\n","\n","Transformer — это сеть, которую придумали в Гугле в конце 2017, и которая сильно побила все рекорды на seq2seq задачах. Её особенность в том, что она не использует никаких hidden state-ов вообще (сама статья называется Attention is All You Nedd).\n","\n","Суть вот в чём: с точки зрения алгоритмики, attention работает за квадрат, а рекуррентные ячейки как бы за линию, но помноженную на рзамерность внутреннего состояния. Transforer применим, когда нам не нужно обрабатывать очень большие последовательности (то есть, на самом деле, почти всегда)."]}]}